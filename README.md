# ğŸ¤– LLM Assistant â€“ VSCode Extension (8INF950)

Extension VSCode dÃ©veloppÃ©e dans le cadre du cours **8INF950 â€“ Sujets spÃ©ciaux**.  
Ce plugiciel intÃ¨gre un **Large Language Model (LLM)** afin dâ€™assister le dÃ©veloppeur directement dans son environnement de dÃ©veloppement.

ğŸ‘¥ **Auteurs**
- Youssef AIT ELOURF  
- Arthur DELHAYE  

---

## ğŸ¯ Objectif du projet

Lâ€™objectif de cette extension est dâ€™automatiser et de faciliter certaines tÃ¢ches du processus de dÃ©veloppement logiciel grÃ¢ce Ã  un LLM, notamment :

- la gÃ©nÃ©ration de code,
- lâ€™explication de code existant,
- la gÃ©nÃ©ration de commentaires et de documentation,
- lâ€™assistance contextuelle directement depuis VSCode.

Lâ€™extension est conÃ§ue pour Ãªtre **simple dâ€™utilisation**, **configurable**, et **robuste**, tout en restant intÃ©grÃ©e de maniÃ¨re native Ã  VSCode.

---

## âœ¨ FonctionnalitÃ©s

### 1ï¸âƒ£ GÃ©nÃ©ration de code
- GÃ©nÃ©ration de code Ã  partir dâ€™une instruction textuelle.
- Insertion automatique du code gÃ©nÃ©rÃ© dans lâ€™Ã©diteur.
- Accessible via :
  - la Webview (sidebar),
  - le menu contextuel,
  - la palette de commandes.

### 2ï¸âƒ£ Explication de code
- Explication claire et concise du code sÃ©lectionnÃ©.
- Affichage de lâ€™explication dans la Webview ou via une notification VSCode.

### 3ï¸âƒ£ Commentaires et documentation
- Ajout automatique de commentaires et de docstrings sur le code sÃ©lectionnÃ©.
- Respect strict de la logique existante (aucune modification fonctionnelle).

---

## ğŸ–¥ï¸ IntÃ©gration VSCode & UX

- Sidebar dÃ©diÃ©e dans la barre dâ€™activitÃ© (**Assistant IA**).
- Menus contextuels dans lâ€™Ã©diteur.
- Palette de commandes VSCode.
- Indicateur de progression lors des appels au LLM.
- Messages dâ€™erreur explicites en cas de problÃ¨me rÃ©seau ou API.

---

## âš™ï¸ PrÃ©requis

- **Node.js** (version recommandÃ©e : LTS)
- **VSCode** (â‰¥ 1.106)
- AccÃ¨s Ã  un LLM compatible via :
  - Ollama (local ou cloud),
  - ou toute API compatible avec lâ€™endpoint configurÃ©.

---

## ğŸ”§ Installation

1. Cloner le dÃ©pÃ´t :
```bash
git clone <url-du-repo>
cd projet-llm-8inf950
```

2. Installer les dÃ©pendances :
```bash
npm install
```

3. Compiler lâ€™extension :
```bash
npm run compile
```

4. Lancer lâ€™extension en mode dÃ©veloppement :
- Ouvrir le projet dans VSCode
- Appuyer sur `F5` (Extension Development Host)

---

## ğŸ”‘ Configuration

Lâ€™extension expose les paramÃ¨tres suivants dans les **Settings VSCode** :

- `llmAssistant.model`  
  ModÃ¨le LLM utilisÃ© (ex. `qwen3-coder`, `llama3`, etc.)

- `llmAssistant.url`  
  URL de lâ€™API LLM (local ou cloud)

- `llmAssistant.apiKey`  
  ClÃ© API si nÃ©cessaire (laisser vide pour un usage local)

---

## ğŸ§ª Tests

Des tests unitaires ont Ã©tÃ© mis en place afin de valider :
- le parsing des rÃ©ponses LLM,
- la logique des handlers,
- la robustesse du service dâ€™appel au modÃ¨le.

Lancer les tests :
```bash
npm test
```

---

## âš ï¸ Limitations connues

- La qualitÃ© des rÃ©ponses dÃ©pend du modÃ¨le LLM configurÃ©.
- Les temps de rÃ©ponse peuvent varier selon la latence rÃ©seau ou la charge du modÃ¨le.
- Les prompts sont gÃ©nÃ©riques et peuvent Ãªtre amÃ©liorÃ©s pour des cas dâ€™usage spÃ©cifiques.

---

## ğŸš€ Pistes dâ€™amÃ©lioration

- Support avancÃ© multi-langage (adaptation fine des prompts).
- Historique des conversations dans la Webview.
- Personnalisation avancÃ©e des paramÃ¨tres du modÃ¨le.
- Ã‰valuation automatique des rÃ©ponses gÃ©nÃ©rÃ©es.

---

## ğŸ“Œ Contexte acadÃ©mique

Projet rÃ©alisÃ© dans le cadre du cours **8INF950 â€“ Sujets spÃ©ciaux**  
DÃ©partement de mathÃ©matiques, dâ€™informatique et de gÃ©nie.

---

**Bonne utilisation !**
